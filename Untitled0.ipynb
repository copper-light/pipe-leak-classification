{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19007,"status":"ok","timestamp":1637107887792,"user":{"displayName":"Donghee Han","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH5BhRXuHTZfTWZ1AtwuoHwRPBWzMyD8Ugy93QVg=s64","userId":"14810394144771630011"},"user_tz":-540},"id":"33qcDWj5O3Vx","outputId":"7d1805bf-e676-42e0-f2f4-fe33363d35d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DzOQFx4gPG2J","outputId":"731475b7-2fd7-4ffb-ec25-46e4f5e1c965"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torchaudio==0.10.0+cu111\n","  Downloading https://download.pytorch.org/whl/cu111/torchaudio-0.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchaudio==0.10.0+cu111) (1.10.0+cu111)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0-\u003etorchaudio==0.10.0+cu111) (3.10.0.2)\n","Installing collected packages: torchaudio\n","Successfully installed torchaudio-0.10.0+cu111\n"]}],"source":["!pip install torchaudio==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n","import os\n","import subprocess\n","train_path = \"/content/train\"\n","test_path = \"/content/test\"\n","cache_file = \"/content/audio.cache\"\n","cache_file_on_drive = \"/content/drive/MyDrive/데이터분석/2_수도관누수데이터/cache/audio.cache\"\n","\n","if os.path.isdir(train_path) == False:\n","    subprocess.run([\"unzip\", \"-qq\", \"/content/drive/MyDrive/데이터분석/dataset.zip\", \"-d\", \"/content\"])"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4286,"status":"ok","timestamp":1637108415117,"user":{"displayName":"Donghee Han","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH5BhRXuHTZfTWZ1AtwuoHwRPBWzMyD8Ugy93QVg=s64","userId":"14810394144771630011"},"user_tz":-540},"id":"kXI56GYFQ9MP","outputId":"06c1246c-e8f3-4c86-c99a-754232c75623"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting wget\n","  Downloading wget-3.2.zip (10 kB)\n","Building wheels for collected packages: wget\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9672 sha256=cd92b3afdfa94c956baf27d9155e9ee809c9176faea749d9b79645405d040ca7\n","  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n","Successfully built wget\n","Installing collected packages: wget\n","Successfully installed wget-3.2\n"]}],"source":["!pip install wget\n","!pip install timm==0.4.5"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1818,"status":"ok","timestamp":1637108262563,"user":{"displayName":"Donghee Han","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH5BhRXuHTZfTWZ1AtwuoHwRPBWzMyD8Ugy93QVg=s64","userId":"14810394144771630011"},"user_tz":-540},"id":"HRR5GB69ORQ8"},"outputs":[],"source":["import os\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import Dataset\n","import torchaudio\n","import numpy as np\n","from sklearn.metrics import f1_score, confusion_matrix\n","import torchvision.models as models\n","import librosa\n","import pandas as pd\n","from torch._utils import _accumulate\n","from torch import default_generator, randperm\n","import random\n","from tqdm import tqdm\n","#import pickle\n","#import shutil\n","import subprocess\n","import logging\n","import argparse"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":529,"status":"ok","timestamp":1637108264274,"user":{"displayName":"Donghee Han","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH5BhRXuHTZfTWZ1AtwuoHwRPBWzMyD8Ugy93QVg=s64","userId":"14810394144771630011"},"user_tz":-540},"id":"6f5V9PfMOdCz"},"outputs":[],"source":["class SoundDataset(Dataset):\n","    \n","    def __init__(self, dataset_path, is_train = True, use_cache = False, cache_file=\"/content/sound_data.cache\", save_cache_file=\"/content/sound_data.cache\"):\n","    #initialize lists to hold file names, labels, and folder numbers\n","        self.file_names = []\n","        self.file_path = []\n","        self.labels = []\n","\n","        self.file_names, self.file_path, self.labels = self._getfile_list(dataset_path)\n","\n","        self.idx_to_label, self.labels_idx = np.unique(self.labels, return_inverse=True)\n","\n","        self.is_train = is_train\n","\n","        self.data = []\n","\n","        self.use_cache = use_cache\n","\n","        if self.use_cache == True:\n","\n","            print(\"Loading dataset into memory..\")\n","            for index in tqdm(range(len(self.file_path))):\n","                self.data.append(self._cache(index))\n","            \n","            # if os.path.isfile(cache_file):\n","            #     with open(cache_file, \"rb\") as file:\n","            #         print(\"Loading cache dataset : {}\".format(cache_file))\n","            #         self.data = pickle.load(file)\n","            # else:\n","            #     print(\"Loading dataset into memory..\")\n","            #     for index in tqdm(range(len(self.file_path))):\n","            #         self.data.append(self._cache(index))\n","\n","            #     print(\"Saving cache to file.. {}\".format(cache_file))\n","            #     with open(cache_file, \"wb\") as file:\n","            #         pickle.dump(self.data, file)    \n","            \n","        \n","\n","    def _cache(self, index):    \n","        n_fft = 256\n","        hop_length = 96\n","        win_length = 256\n","        n_mels = 224\n","        sample_rate = 22050\n","        \n","        path = self.file_path[index]\n","        waveform, sample = torchaudio.load(path)\n","        waveform = torchaudio.transforms.Resample(sample, sample_rate)(waveform)[0]\n","\n","        if self.is_train:\n","            self.transform = transforms.Compose([\n","                    #transforms.Normalize(mean=[0.485], std=[0.229]), \n","                    transforms.Resize((150, 128)),\n","                    #transforms.RandomHorizontalFlip()\n","            ])\n","            #waveform = self.audio_trans.apply((waveform.numpy(), sample_rate))[0].type(torch.FloatTensor)\n","            #print(waveform.shape)\n","        else:\n","            self.transform = transforms.Compose([\n","                    # transforms.Normalize(mean=[0.485], std=[0.229]),\n","                    transforms.Resize((150, 128)),\n","            ])\n","        \n","        #waveform = waveform[0, int(sample_rate/5):len(waveform) - int(sample_rate/5)]\n","\n","        # waveform[waveform \u003e 0.0015] = 0.0015\n","        # waveform[waveform \u003c -0.002] = -0.002\n","\n","        # while(len(waveform) \u003c 22050*5):\n","        #     waveform = torch.cat([waveform, waveform[0: max(len(waveform), 22050*5 - len(waveform))]], dim=0)\n","\n","        #waveform = torchaudio.functional.filtering.vad(waveform, sample_rate=sample, noise_reduction_amount= 3)[0]\n","\n","        #음향신호를 시각화 x=시간, y=주파수, z=진폭\n","        #waveform = torchaudio.functional.gain(waveform, gain_db=200.0) # best 200\n","\n","        #waveform_low = torchaudio.functional.lowpass_biquad(waveform, sample_rate=sample, cutoff_freq=3000)\n","        specgram1 = torchaudio.transforms.Spectrogram(n_fft=n_fft, hop_length=hop_length, win_length=win_length)(waveform)\n","        specgram1 = self._normalize(specgram1).unsqueeze(dim=0)\n","        specgram1 = self.transform(specgram1).squeeze(dim=0)\n","        specgram1 = torch.transpose(specgram1,0,1)\n","        # 음성신호를 캐치하기위한 스펙트로그램\n","\n","        #waveform_low = torchaudio.functional.lowpass_biquad(waveform, sample_rate=sample, cutoff_freq=3000)\n","#         specgram2 = torchaudio.transforms.MelSpectrogram(n_mels=n_mels)(waveform)\n","#         specgram2 = self._normalize(specgram2).unsqueeze(dim=0)\n","#         specgram2 = self.transform(specgram2).squeeze(dim=0)\n","\n","#         waveform5 = waveform.numpy()\n","#         specgram5 = librosa.feature.mfcc(waveform5, sr=sample, n_mfcc = 128)\n","#         specgram5 = torch.tensor(specgram5)\n","#         specgram5 = self._normalize(specgram5).unsqueeze(dim=0)\n","#         specgram5 = self.transform(specgram5).squeeze(dim=0)\n","\n","        #waveform2 = torchaudio.functional.gain(waveform, gain_db=500.0)\n","        # specgram5 = torchaudio.transforms.MuLawEncoding()(waveform).float()\n","        # specgram5 = torchaudio.transforms.Spectrogram()(specgram5)\n","        # specgram5 = self._normalize(specgram5).unsqueeze(dim=0)\n","        # specgram5 = self.transform(specgram5).squeeze(dim=0)\n","\n","        # # waveform2 = torchaudio.functional.filtering.vad(waveform.unsqueeze(dim=0), sample_rate=sample, noise_reduction_amount= 1.5)[0]\n","        # # specgram2 = torchaudio.transforms.Spectrogram()(waveform2)\n","        # # specgram2 = self._normalize(specgram2).unsqueeze(dim=0)\n","        # # specgram2 = self.transform(specgram2).squeeze(dim=0)\n","      \n","        # waveform3 = waveform.numpy()\n","        # specgram3 = np.abs(librosa.stft(waveform3, n_fft=1024, hop_length=512))\n","        # specgram3 = librosa.amplitude_to_db(specgram3, ref=np.mean)\n","        # specgram3 = torch.tensor(specgram3)\n","        # specgram3 = specgram3.log2()\n","        # specgram3 = self._normalize(specgram3).unsqueeze(dim=0)\n","        # specgram3 = self.transform(specgram3).squeeze(dim=0)\n","\n","        # #specgram4 = torchaudio.transforms.MelSpectrogram(sample_rate=sample, n_fft=1024, n_mels=512)(waveform)\n","        # specgram4 = torchaudio.transforms.MelSpectrogram(sample_rate=sample, n_fft=1024, n_mels=64)(waveform)\n","        # specgram4 = specgram4.log2()\n","        # specgram4 = self._normalize(specgram4).unsqueeze(dim=0)\n","        # specgram4 = self.transform(specgram4).squeeze(dim=0)\n","\n","        # waveform5 = waveform.numpy()\n","        # specgram5 = librosa.feature.mfcc(waveform5, sr=sample, n_mfcc = 224)\n","        # specgram5 = torch.tensor(specgram5)\n","        # specgram5 = specgram5.log2()\n","        # specgram5 = self._normalize(specgram5).unsqueeze(dim=0)\n","        # specgram5 = self.transform(specgram5).squeeze(dim=0)\n","\n","        #specgram = torch.stack([specgram1,specgram2,specgram3], dim=0)\n","\n","        \n","\n","        # specgram4 = torchaudio.transforms.Spectrogram()(f_waveform)\n","        # specgram4 = self._normalize(specgram4).unsqueeze(dim=0)\n","        # specgram4 = self.transform(specgram4).squeeze(dim=0)\n","\n","        # waveform5 = torchaudio.functional.gain(f_waveform, gain_db=500.0)\n","        # waveform5 = torchaudio.transforms.MuLawEncoding()(waveform5).float()\n","        # specgram5 = torchaudio.transforms.Spectrogram()(waveform5)\n","        # specgram5 = self._normalize(specgram5).unsqueeze(dim=0)\n","        # specgram5 = self.transform(specgram5).squeeze(dim=0)\n","      \n","        # waveform6 = f_waveform.numpy()\n","        # specgram6 = np.abs(librosa.stft(waveform6, n_fft=1024, hop_length=512))\n","        # specgram6 = librosa.amplitude_to_db(specgram6, ref=np.max)\n","        # specgram6 = torch.tensor(specgram6)\n","        # specgram6 = self._normalize(specgram6).unsqueeze(dim=0)\n","        # specgram6 = self.transform(specgram6).squeeze(dim=0)\n","\n","        #specgram = torch.stack([specgram1], dim=0) # ,specgram2, specgram3,specgram4,specgram5\n","\n","        return specgram1\n","\n","    def train(self):\n","        self.is_train = True\n","\n","    def eval(self):\n","        self.is_train = False\n","\n","    def _getfile_list(self, path, parent=\"\"):\n","        file_paths = []\n","        file_names = []\n","        labels = []\n","        for filename in os.listdir(path):\n","            fullpath = os.path.join(path, filename)\n","            if os.path.isfile(fullpath):\n","                if (parent is \"\"): parent = \"None\"\n","                file_paths.append(fullpath)\n","                file_names.append(filename)\n","                labels.append(parent)\n","            else:\n","                f, p, l = self._getfile_list(fullpath, filename)\n","                file_names += f\n","                file_paths += p\n","                labels += l\n","        return file_names, file_paths, labels\n","    \n","    def _normalize(self, tensor):\n","        tensor_minusmean = tensor - tensor.mean()\n","        return tensor/tensor_minusmean.abs().max()\n","    \n","    \n","    def __getitem__(self, index):\n","        data = None\n","        if self.use_cache:\n","            data = self.data[index]\n","        else:\n","            data = self._cache(index)\n","\n","        return data, self.file_names[index], self.labels_idx[index]\n","\n","    def __len__(self):\n","        return len(self.file_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"HB-s5bbUQ5bY"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n","  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"]},{"name":"stdout","output_type":"stream","text":["---------------AST Model Summary---------------\n","ImageNet pretraining: True, AudioSet pretraining: False\n"]},{"name":"stderr","output_type":"stream","text":["Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_384-d0272ac0.pth\" to ../../pretrained_models/hub/checkpoints/deit_base_distilled_patch16_384-d0272ac0.pth\n"]},{"name":"stdout","output_type":"stream","text":["frequncey stride=10, time stride=10\n","number of patches=108\n","torch.Size([10, 527])\n","---------------AST Model Summary---------------\n","ImageNet pretraining: True, AudioSet pretraining: True\n","frequncey stride=10, time stride=10\n","number of patches=300\n","torch.Size([10, 50])\n"]}],"source":["\n","import torch\n","import torch.nn as nn\n","from torch.cuda.amp import autocast\n","import os\n","import wget\n","os.environ['TORCH_HOME'] = '../../pretrained_models'\n","import timm\n","from timm.models.layers import to_2tuple,trunc_normal_\n","\n","# override the timm package to relax the input shape constraint.\n","class PatchEmbed(nn.Module):\n","    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n","        super().__init__()\n","\n","        img_size = to_2tuple(img_size)\n","        patch_size = to_2tuple(patch_size)\n","        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.num_patches = num_patches\n","\n","        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n","\n","    def forward(self, x):\n","        x = self.proj(x).flatten(2).transpose(1, 2)\n","        return x\n","\n","class ASTModel(nn.Module):\n","    \"\"\"\n","    The AST model.\n","    :param label_dim: the label dimension, i.e., the number of total classes, it is 527 for AudioSet, 50 for ESC-50, and 35 for speechcommands v2-35\n","    :param fstride: the stride of patch spliting on the frequency dimension, for 16*16 patchs, fstride=16 means no overlap, fstride=10 means overlap of 6\n","    :param tstride: the stride of patch spliting on the time dimension, for 16*16 patchs, tstride=16 means no overlap, tstride=10 means overlap of 6\n","    :param input_fdim: the number of frequency bins of the input spectrogram\n","    :param input_tdim: the number of time frames of the input spectrogram\n","    :param imagenet_pretrain: if use ImageNet pretrained model\n","    :param audioset_pretrain: if use full AudioSet and ImageNet pretrained model\n","    :param model_size: the model size of AST, should be in [tiny224, small224, base224, base384], base224 and base 384 are same model, but are trained differently during ImageNet pretraining.\n","    \"\"\"\n","    def __init__(self, label_dim=527, fstride=10, tstride=10, input_fdim=128, input_tdim=1024, imagenet_pretrain=True, audioset_pretrain=False, model_size='base384', verbose=True):\n","\n","        super(ASTModel, self).__init__()\n","        assert timm.__version__ == '0.4.5', 'Please use timm == 0.4.5, the code might not be compatible with newer versions.'\n","\n","        if verbose == True:\n","            print('---------------AST Model Summary---------------')\n","            print('ImageNet pretraining: {:s}, AudioSet pretraining: {:s}'.format(str(imagenet_pretrain),str(audioset_pretrain)))\n","        # override timm input shape restriction\n","        timm.models.vision_transformer.PatchEmbed = PatchEmbed\n","\n","        # if AudioSet pretraining is not used (but ImageNet pretraining may still apply)\n","        if audioset_pretrain == False:\n","            if model_size == 'tiny224':\n","                self.v = timm.create_model('vit_deit_tiny_distilled_patch16_224', pretrained=imagenet_pretrain)\n","            elif model_size == 'small224':\n","                self.v = timm.create_model('vit_deit_small_distilled_patch16_224', pretrained=imagenet_pretrain)\n","            elif model_size == 'base224':\n","                self.v = timm.create_model('vit_deit_base_distilled_patch16_224', pretrained=imagenet_pretrain)\n","            elif model_size == 'base384':\n","                self.v = timm.create_model('vit_deit_base_distilled_patch16_384', pretrained=imagenet_pretrain)\n","            else:\n","                raise Exception('Model size must be one of tiny224, small224, base224, base384.')\n","            self.original_num_patches = self.v.patch_embed.num_patches\n","            self.oringal_hw = int(self.original_num_patches ** 0.5)\n","            self.original_embedding_dim = self.v.pos_embed.shape[2]\n","            self.mlp_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim), nn.Linear(self.original_embedding_dim, label_dim))\n","\n","            # automatcially get the intermediate shape\n","            f_dim, t_dim = self.get_shape(fstride, tstride, input_fdim, input_tdim)\n","            num_patches = f_dim * t_dim\n","            self.v.patch_embed.num_patches = num_patches\n","            if verbose == True:\n","                print('frequncey stride={:d}, time stride={:d}'.format(fstride, tstride))\n","                print('number of patches={:d}'.format(num_patches))\n","\n","            # the linear projection layer\n","            new_proj = torch.nn.Conv2d(1, self.original_embedding_dim, kernel_size=(16, 16), stride=(fstride, tstride))\n","            if imagenet_pretrain == True:\n","                new_proj.weight = torch.nn.Parameter(torch.sum(self.v.patch_embed.proj.weight, dim=1).unsqueeze(1))\n","                new_proj.bias = self.v.patch_embed.proj.bias\n","            self.v.patch_embed.proj = new_proj\n","\n","            # the positional embedding\n","            if imagenet_pretrain == True:\n","                # get the positional embedding from deit model, skip the first two tokens (cls token and distillation token), reshape it to original 2D shape (24*24).\n","                new_pos_embed = self.v.pos_embed[:, 2:, :].detach().reshape(1, self.original_num_patches, self.original_embedding_dim).transpose(1, 2).reshape(1, self.original_embedding_dim, self.oringal_hw, self.oringal_hw)\n","                # cut (from middle) or interpolate the second dimension of the positional embedding\n","                if t_dim \u003c= self.oringal_hw:\n","                    new_pos_embed = new_pos_embed[:, :, :, int(self.oringal_hw / 2) - int(t_dim / 2): int(self.oringal_hw / 2) - int(t_dim / 2) + t_dim]\n","                else:\n","                    new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(self.oringal_hw, t_dim), mode='bilinear')\n","                # cut (from middle) or interpolate the first dimension of the positional embedding\n","                if f_dim \u003c= self.oringal_hw:\n","                    new_pos_embed = new_pos_embed[:, :, int(self.oringal_hw / 2) - int(f_dim / 2): int(self.oringal_hw / 2) - int(f_dim / 2) + f_dim, :]\n","                else:\n","                    new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(f_dim, t_dim), mode='bilinear')\n","                # flatten the positional embedding\n","                new_pos_embed = new_pos_embed.reshape(1, self.original_embedding_dim, num_patches).transpose(1,2)\n","                # concatenate the above positional embedding with the cls token and distillation token of the deit model.\n","                self.v.pos_embed = nn.Parameter(torch.cat([self.v.pos_embed[:, :2, :].detach(), new_pos_embed], dim=1))\n","            else:\n","                # if not use imagenet pretrained model, just randomly initialize a learnable positional embedding\n","                # TODO can use sinusoidal positional embedding instead\n","                new_pos_embed = nn.Parameter(torch.zeros(1, self.v.patch_embed.num_patches + 2, self.original_embedding_dim))\n","                self.v.pos_embed = new_pos_embed\n","                trunc_normal_(self.v.pos_embed, std=.02)\n","\n","        # now load a model that is pretrained on both ImageNet and AudioSet\n","        elif audioset_pretrain == True:\n","            if audioset_pretrain == True and imagenet_pretrain == False:\n","                raise ValueError('currently model pretrained on only audioset is not supported, please set imagenet_pretrain = True to use audioset pretrained model.')\n","            if model_size != 'base384':\n","                raise ValueError('currently only has base384 AudioSet pretrained model.')\n","            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","            if os.path.exists('../../pretrained_models/audioset_10_10_0.4593.pth') == False:\n","                # this model performs 0.4593 mAP on the audioset eval set\n","                audioset_mdl_url = 'https://www.dropbox.com/s/cv4knew8mvbrnvq/audioset_0.4593.pth?dl=1'\n","                wget.download(audioset_mdl_url, out='../../pretrained_models/audioset_10_10_0.4593.pth')\n","            sd = torch.load('../../pretrained_models/audioset_10_10_0.4593.pth', map_location=device)\n","            audio_model = ASTModel(label_dim=527, fstride=10, tstride=10, input_fdim=128, input_tdim=1024, imagenet_pretrain=False, audioset_pretrain=False, model_size='base384', verbose=False)\n","            audio_model = torch.nn.DataParallel(audio_model)\n","            audio_model.load_state_dict(sd, strict=False)\n","            self.v = audio_model.module.v\n","            self.original_embedding_dim = self.v.pos_embed.shape[2]\n","            self.mlp_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim), nn.Linear(self.original_embedding_dim, label_dim))\n","\n","            f_dim, t_dim = self.get_shape(fstride, tstride, input_fdim, input_tdim)\n","            num_patches = f_dim * t_dim\n","            self.v.patch_embed.num_patches = num_patches\n","            if verbose == True:\n","                print('frequncey stride={:d}, time stride={:d}'.format(fstride, tstride))\n","                print('number of patches={:d}'.format(num_patches))\n","\n","            new_pos_embed = self.v.pos_embed[:, 2:, :].detach().reshape(1, 1212, 768).transpose(1, 2).reshape(1, 768, 12, 101)\n","            # if the input sequence length is larger than the original audioset (10s), then cut the positional embedding\n","            if t_dim \u003c 101:\n","                new_pos_embed = new_pos_embed[:, :, :, 50 - int(t_dim/2): 50 - int(t_dim/2) + t_dim]\n","            # otherwise interpolate\n","            else:\n","                new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(12, t_dim), mode='bilinear')\n","            new_pos_embed = new_pos_embed.reshape(1, 768, num_patches).transpose(1, 2)\n","            self.v.pos_embed = nn.Parameter(torch.cat([self.v.pos_embed[:, :2, :].detach(), new_pos_embed], dim=1))\n","\n","    def get_shape(self, fstride, tstride, input_fdim=128, input_tdim=1024):\n","        test_input = torch.randn(1, 1, input_fdim, input_tdim)\n","        test_proj = nn.Conv2d(1, self.original_embedding_dim, kernel_size=(16, 16), stride=(fstride, tstride))\n","        test_out = test_proj(test_input)\n","        f_dim = test_out.shape[2]\n","        t_dim = test_out.shape[3]\n","        return f_dim, t_dim\n","\n","    @autocast()\n","    def forward(self, x):\n","        \"\"\"\n","        :param x: the input spectrogram, expected shape: (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n","        :return: prediction\n","        \"\"\"\n","        # expect input x = (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n","        x = x.unsqueeze(1)\n","        x = x.transpose(2, 3)\n","\n","        B = x.shape[0]\n","        x = self.v.patch_embed(x)\n","        cls_tokens = self.v.cls_token.expand(B, -1, -1)\n","        dist_token = self.v.dist_token.expand(B, -1, -1)\n","        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n","        x = x + self.v.pos_embed\n","        x = self.v.pos_drop(x)\n","        for blk in self.v.blocks:\n","            x = blk(x)\n","        x = self.v.norm(x)\n","        x = (x[:, 0] + x[:, 1]) / 2\n","\n","        x = self.mlp_head(x)\n","        return x\n","\n","if __name__ == '__main__':\n","    input_tdim = 100\n","    ast_mdl = ASTModel(input_tdim=input_tdim)\n","    # input a batch of 10 spectrogram, each with 100 time frames and 128 frequency bins\n","    test_input = torch.rand([10, input_tdim, 128])\n","    test_output = ast_mdl(test_input)\n","    # output should be in shape [10, 527], i.e., 10 samples, each with prediction of 527 classes.\n","    print(test_output.shape)\n","\n","    input_tdim = 256\n","    ast_mdl = ASTModel(input_tdim=input_tdim,label_dim=50, audioset_pretrain=True)\n","    # input a batch of 10 spectrogram, each with 512 time frames and 128 frequency bins\n","    test_input = torch.rand([10, input_tdim, 128])\n","    test_output = ast_mdl(test_input)\n","    # output should be in shape [10, 50], i.e., 10 samples, each with prediction of 50 classes.\n","    print(test_output.shape)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":368},"executionInfo":{"elapsed":329,"status":"error","timestamp":1637108348670,"user":{"displayName":"Donghee Han","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH5BhRXuHTZfTWZ1AtwuoHwRPBWzMyD8Ugy93QVg=s64","userId":"14810394144771630011"},"user_tz":-540},"id":"cjJbXzjAO6J7","outputId":"67e15ddb-69b7-47b8-8a5f-7badec0a99cf"},"outputs":[{"ename":"ImportError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-9-209422da4c8f\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mast_models\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# download pretrained model in this directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TORCH_HOME'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../pretrained_models'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}],"source":["import os\n","import torch\n","from .ast_models import *\n","# download pretrained model in this directory\n","os.environ['TORCH_HOME'] = '../pretrained_models'\n","# assume each input spectrogram has 100 time frames\n","input_tdim = 150\n","# assume the task has 527 classes\n","label_dim = 1\n","# create a pseudo input: a batch of 10 spectrogram, each with 100 time frames and 128 frequency bins\n","test_input = torch.rand([10, input_tdim, 128]).half().cuda()\n","# create an AST model\n","ast_mdl = ASTModel(label_dim=label_dim, input_tdim=input_tdim, imagenet_pretrain=True, audioset_pretrain=True).cuda()\n","test_output = ast_mdl(test_input)\n","# output should be in shape [10, 527], i.e., 10 samples, each with prediction of 527 classes.\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":343,"status":"ok","timestamp":1637108271520,"user":{"displayName":"Donghee Han","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH5BhRXuHTZfTWZ1AtwuoHwRPBWzMyD8Ugy93QVg=s64","userId":"14810394144771630011"},"user_tz":-540},"id":"BFGT4cIdPPkh","outputId":"0a6cadae-d8b8-4eb5-aa52-9e1145e8ee0b"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n"]}],"source":["!pwd\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6G-SB6nPPwjD"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMt0YBM9vzGr2YCw0TR+wUV","mount_file_id":"1NUtSkFGmKK9w5p8S-bhjtKIF9iZamNgW","name":"Untitled0.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}